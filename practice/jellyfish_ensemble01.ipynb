{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1xWJy5L7ylviZkb2MSul4eYVo6LnmVh0J",
      "authorship_tag": "ABX9TyNrRMrXubeI1sz9d8uKmWj4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/euns-tory/AIFFEL_quest_cr/blob/main/practice/jellyfish_ensemble01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKur6eRpTNvi",
        "outputId": "cbac7845-7091-4ef9-9a08-eba69c5960fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv /content/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "YyIJIcK6U2Oh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/jellyfish"
      ],
      "metadata": {
        "id": "9yhvGp6kV22i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d anshtanwar/jellyfish-types -p /content/jellyfish"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YWDHkHSV9dW",
        "outputId": "4a2b50ba-99cc-49c5-eadf-08311d2a6b0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/anshtanwar/jellyfish-types\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/jellyfish/jellyfish-types.zip -d /content/jellyfish/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F6YIpRvfWDJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras"
      ],
      "metadata": {
        "id": "4d9-oyM-WLkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50, EfficientNetB0, MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 데이터 경로 설정\n",
        "data_dir = \"/content/jellyfish/\"\n",
        "\n",
        "# 이미지 크기 및 하이퍼파라미터 설정\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# 데이터 증강 및 로드\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "num_classes = len(train_generator.class_indices)\n",
        "\n",
        "# 기본 모델 생성 함수\n",
        "def create_model(base_model):\n",
        "    base_model.trainable = False  # 사전 학습된 가중치 고정\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    return model\n",
        "\n",
        "# 개별 모델 정의\n",
        "base_models = {\n",
        "    \"ResNet50\": ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)),\n",
        "    \"EfficientNetB0\": EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)),\n",
        "    \"MobileNetV2\": MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "}\n",
        "\n",
        "models = {}\n",
        "for name, base in base_models.items():\n",
        "    model = create_model(base)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    models[name] = model\n",
        "\n",
        "# 개별 모델 학습\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(train_generator, validation_data=val_generator, epochs=epochs)\n",
        "\n",
        "# 앙상블 예측 함수 (Soft Voting)\n",
        "def ensemble_predict(models, data):\n",
        "    predictions = [model.predict(data) for model in models.values()]\n",
        "    avg_prediction = np.mean(predictions, axis=0)  # Soft Voting (확률 평균)\n",
        "    return avg_prediction\n",
        "\n",
        "# 모델 평가 (앙상블 적용)\n",
        "val_data, val_labels = next(val_generator)\n",
        "ensemble_preds = ensemble_predict(models, val_data)\n",
        "ensemble_labels = np.argmax(ensemble_preds, axis=1)\n",
        "true_labels = np.argmax(val_labels, axis=1)\n",
        "\n",
        "accuracy = np.mean(ensemble_labels == true_labels)\n",
        "print(f\"앙상블 모델 정확도: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9XVH1o7X1vC",
        "outputId": "d5a853da-7c9b-4fa3-cecc-75d62def02e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1504 images belonging to 7 classes.\n",
            "Found 375 images belonging to 7 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training ResNet50...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 8s/step - accuracy: 0.2448 - loss: 2.0850 - val_accuracy: 0.5200 - val_loss: 1.5534\n",
            "Epoch 2/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 8s/step - accuracy: 0.4958 - loss: 1.6248 - val_accuracy: 0.5200 - val_loss: 1.5511\n",
            "Epoch 3/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 7s/step - accuracy: 0.5164 - loss: 1.6297 - val_accuracy: 0.5200 - val_loss: 1.5513\n",
            "Epoch 4/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 8s/step - accuracy: 0.5204 - loss: 1.5910 - val_accuracy: 0.5200 - val_loss: 1.5494\n",
            "Epoch 5/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 7s/step - accuracy: 0.5196 - loss: 1.6091 - val_accuracy: 0.5200 - val_loss: 1.5510\n",
            "Epoch 6/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 8s/step - accuracy: 0.5274 - loss: 1.5893 - val_accuracy: 0.5200 - val_loss: 1.5519\n",
            "Epoch 7/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 8s/step - accuracy: 0.4986 - loss: 1.6371 - val_accuracy: 0.5200 - val_loss: 1.5499\n",
            "Epoch 8/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 7s/step - accuracy: 0.5178 - loss: 1.5817 - val_accuracy: 0.5200 - val_loss: 1.5504\n",
            "Epoch 9/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 7s/step - accuracy: 0.5030 - loss: 1.6054 - val_accuracy: 0.5200 - val_loss: 1.5488\n",
            "Epoch 10/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 8s/step - accuracy: 0.5306 - loss: 1.5739 - val_accuracy: 0.5200 - val_loss: 1.5503\n",
            "Training EfficientNetB0...\n",
            "Epoch 1/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 3s/step - accuracy: 0.4257 - loss: 1.7061 - val_accuracy: 0.5200 - val_loss: 1.5534\n",
            "Epoch 2/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 3s/step - accuracy: 0.5205 - loss: 1.6140 - val_accuracy: 0.5200 - val_loss: 1.5528\n",
            "Epoch 3/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 3s/step - accuracy: 0.5201 - loss: 1.5861 - val_accuracy: 0.5200 - val_loss: 1.5534\n",
            "Epoch 4/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 3s/step - accuracy: 0.5329 - loss: 1.5649 - val_accuracy: 0.5200 - val_loss: 1.5558\n",
            "Epoch 5/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 4s/step - accuracy: 0.5050 - loss: 1.6117 - val_accuracy: 0.5200 - val_loss: 1.5528\n",
            "Epoch 6/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 3s/step - accuracy: 0.5121 - loss: 1.5980 - val_accuracy: 0.5200 - val_loss: 1.5536\n",
            "Epoch 7/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 3s/step - accuracy: 0.5285 - loss: 1.5606 - val_accuracy: 0.5200 - val_loss: 1.5533\n",
            "Epoch 8/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 3s/step - accuracy: 0.5244 - loss: 1.5664 - val_accuracy: 0.5200 - val_loss: 1.5565\n",
            "Epoch 9/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 3s/step - accuracy: 0.5177 - loss: 1.6021 - val_accuracy: 0.5200 - val_loss: 1.5528\n",
            "Epoch 10/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 3s/step - accuracy: 0.5109 - loss: 1.5990 - val_accuracy: 0.5200 - val_loss: 1.5536\n",
            "Training MobileNetV2...\n",
            "Epoch 1/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.2636 - loss: 2.2911 - val_accuracy: 0.4907 - val_loss: 1.6370\n",
            "Epoch 2/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - accuracy: 0.4576 - loss: 1.6164 - val_accuracy: 0.4240 - val_loss: 1.5255\n",
            "Epoch 3/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 2s/step - accuracy: 0.4743 - loss: 1.4630 - val_accuracy: 0.3840 - val_loss: 1.4683\n",
            "Epoch 4/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.5107 - loss: 1.3685 - val_accuracy: 0.3547 - val_loss: 1.4680\n",
            "Epoch 5/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.5425 - loss: 1.2577 - val_accuracy: 0.3093 - val_loss: 1.4729\n",
            "Epoch 6/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - accuracy: 0.5318 - loss: 1.1895 - val_accuracy: 0.2987 - val_loss: 1.4723\n",
            "Epoch 7/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.5218 - loss: 1.1823 - val_accuracy: 0.2720 - val_loss: 1.5215\n",
            "Epoch 8/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.5178 - loss: 1.1637 - val_accuracy: 0.2773 - val_loss: 1.4998\n",
            "Epoch 9/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.5444 - loss: 1.0969 - val_accuracy: 0.2613 - val_loss: 1.5122\n",
            "Epoch 10/10\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - accuracy: 0.5545 - loss: 1.0388 - val_accuracy: 0.2400 - val_loss: 1.5711\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "앙상블 모델 정확도: 43.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fPfd8uttX81B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}